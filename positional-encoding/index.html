<!DOCTYPE html><html lang="zh-CN"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/positional-encoding.svg"/><link rel="stylesheet" href="/transformer-paper-site/_next/static/css/074657df094ca3ac.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/transformer-paper-site/_next/static/chunks/webpack-1c2f0ab7e589b28f.js"/><script src="/transformer-paper-site/_next/static/chunks/96e220d1-dd08d031967947a2.js" async=""></script><script src="/transformer-paper-site/_next/static/chunks/770-131adaed12890c69.js" async=""></script><script src="/transformer-paper-site/_next/static/chunks/main-app-440889e68b6317e3.js" async=""></script><script src="/transformer-paper-site/_next/static/chunks/91-e2c009cf22ee4bab.js" async=""></script><script src="/transformer-paper-site/_next/static/chunks/690-1ea658c9bde215d9.js" async=""></script><script src="/transformer-paper-site/_next/static/chunks/app/positional-encoding/page-a97ea86f67e7e4ac.js" async=""></script><title>Transformer论文解析</title><meta name="description" content="《Attention Is All You Need》论文详细解析，了解Transformer架构的核心创新和工作原理"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/transformer-paper-site/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body><div class="jsx-a8308ad811a3dff5 layout"><button aria-label="打开菜单" class="jsx-6dbfbfbce4844938 sidebar-toggle"><span class="jsx-6dbfbfbce4844938"></span><span class="jsx-6dbfbfbce4844938"></span><span class="jsx-6dbfbfbce4844938"></span></button><aside class="jsx-6dbfbfbce4844938 sidebar "><div class="jsx-6dbfbfbce4844938 sidebar-header"><h2 class="jsx-6dbfbfbce4844938">目录</h2></div><nav class="jsx-6dbfbfbce4844938 sidebar-nav"><ul class="jsx-6dbfbfbce4844938"><li class="jsx-6dbfbfbce4844938"><a href="#overview">论文概述</a></li><li class="jsx-6dbfbfbce4844938"><a href="#innovations">关键创新点</a></li><li class="jsx-6dbfbfbce4844938"><a href="#architecture">Transformer架构详解</a></li><li class="jsx-6dbfbfbce4844938"><a href="#components">核心组件详解</a></li><li class="jsx-6dbfbfbce4844938"><a href="#advantages">Transformer的优势</a></li><li class="jsx-6dbfbfbce4844938"><a href="#results">实验结果</a></li><li class="jsx-6dbfbfbce4844938"><a href="#variants">模型变体实验</a></li><li class="jsx-6dbfbfbce4844938"><a href="#impact">Transformer的影响与应用</a></li><li class="jsx-6dbfbfbce4844938"><a href="#conclusion">总结</a></li></ul></nav><div class="jsx-6dbfbfbce4844938 sidebar-footer"><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer" class="jsx-6dbfbfbce4844938">原论文链接</a></div></aside><header class="jsx-dd9b86782d437359 header"><div class="jsx-dd9b86782d437359 container header-container"><div class="jsx-dd9b86782d437359 logo"><a href="/transformer-paper-site/"><span class="jsx-dd9b86782d437359 logo-text">Transformer论文解析</span></a></div><nav class="jsx-dd9b86782d437359 nav"><a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener noreferrer" class="jsx-dd9b86782d437359">GitHub源码</a><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer" class="jsx-dd9b86782d437359">原论文</a></nav></div></header><main class="jsx-a8308ad811a3dff5 main-content"><div class="jsx-a8308ad811a3dff5 container"><article class="jsx-a8308ad811a3dff5 encoding-content"><h1 class="jsx-a8308ad811a3dff5">位置编码详解</h1><section class="jsx-a8308ad811a3dff5"><h2 class="jsx-a8308ad811a3dff5">位置编码的作用</h2><p class="jsx-a8308ad811a3dff5">由于Transformer不使用循环和卷积结构，它无法感知输入序列中的位置信息。 为了让模型了解序列中的位置信息，Transformer引入了位置编码（Positional Encoding）。 位置编码将位置信息编码到输入的词向量中，使得模型能够学习和利用序列的顺序信息。</p></section><section class="jsx-a8308ad811a3dff5"><h2 class="jsx-a8308ad811a3dff5">正弦和余弦函数位置编码</h2><p class="jsx-a8308ad811a3dff5">Transformer使用了基于正弦和余弦函数的位置编码方法，其公式如下：</p><div class="jsx-f15f37c580e7221a code-block"><pre class="jsx-f15f37c580e7221a"><code class="jsx-f15f37c580e7221a language-python">PE(pos, 2i) = sin(pos/10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))</code></pre></div><p class="jsx-a8308ad811a3dff5">其中：</p><ul class="jsx-a8308ad811a3dff5"><li class="jsx-a8308ad811a3dff5"><strong class="jsx-a8308ad811a3dff5">pos</strong>：表示单词在序列中的位置</li><li class="jsx-a8308ad811a3dff5"><strong class="jsx-a8308ad811a3dff5">i</strong>：表示位置编码的维度索引</li><li class="jsx-a8308ad811a3dff5"><strong class="jsx-a8308ad811a3dff5">d_model</strong>：模型的隐藏层维度，与词嵌入维度相同</li></ul><div class="jsx-a8308ad811a3dff5 image-container"><img alt="位置编码可视化" width="700" height="400" decoding="async" data-nimg="1" style="color:transparent" src="/positional-encoding.svg"/><p class="jsx-a8308ad811a3dff5 image-caption">图1：位置编码矩阵可视化（颜色表示不同维度的值）</p></div></section><section class="jsx-a8308ad811a3dff5"><h2 class="jsx-a8308ad811a3dff5">位置编码的特性</h2><p class="jsx-a8308ad811a3dff5">论文选择正弦和余弦函数作为位置编码的原因主要有：</p><ol class="jsx-a8308ad811a3dff5"><li class="jsx-a8308ad811a3dff5"><strong class="jsx-a8308ad811a3dff5">相对位置感知能力</strong>：对于固定偏移k，PE(pos+k)可以表示为PE(pos)的线性函数， 这使得模型可以轻松学习关注相对位置。</li><li class="jsx-a8308ad811a3dff5"><strong class="jsx-a8308ad811a3dff5">无限长度外推</strong>：正弦和余弦函数可以外推到训练时未见过的序列长度。</li><li class="jsx-a8308ad811a3dff5"><strong class="jsx-a8308ad811a3dff5">唯一性</strong>：每个位置对应一个唯一的编码向量。</li></ol><p class="jsx-a8308ad811a3dff5">在实际应用中，位置编码向量被添加到输入的词嵌入中：</p><div class="jsx-f15f37c580e7221a code-block"><pre class="jsx-f15f37c580e7221a"><code class="jsx-f15f37c580e7221a language-python">输入表示 = 词嵌入 + 位置编码</code></pre></div></section><section class="jsx-a8308ad811a3dff5"><h2 class="jsx-a8308ad811a3dff5">位置编码的变体</h2><p class="jsx-a8308ad811a3dff5">除了原始论文中使用的固定正弦位置编码外，还有其他类型的位置编码方法：</p><ul class="jsx-a8308ad811a3dff5"><li class="jsx-a8308ad811a3dff5"><strong class="jsx-a8308ad811a3dff5">可学习的位置嵌入</strong>：与词嵌入类似，直接学习每个位置的向量表示。 论文的消融实验中提到，可学习的位置嵌入与固定的正弦位置编码效果相当。</li><li class="jsx-a8308ad811a3dff5"><strong class="jsx-a8308ad811a3dff5">相对位置编码</strong>：后续改进的Transformer模型如Transformer-XL引入了相对位置编码， 仅考虑词与词之间的相对距离，而非绝对位置。</li></ul></section><div class="jsx-a8308ad811a3dff5 navigation-links"><a class="nav-link" href="/transformer-paper-site/">返回首页</a><a class="nav-link" href="/transformer-paper-site/self-attention/">自注意力机制</a></div></article></div></main></div><!--$--><!--/$--><!--$--><!--/$--><script src="/transformer-paper-site/_next/static/chunks/webpack-1c2f0ab7e589b28f.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[831,[],\"\"]\n3:I[8307,[],\"\"]\n4:I[9226,[],\"ClientPageRoot\"]\n5:I[8994,[\"91\",\"static/chunks/91-e2c009cf22ee4bab.js\",\"690\",\"static/chunks/690-1ea658c9bde215d9.js\",\"701\",\"static/chunks/app/positional-encoding/page-a97ea86f67e7e4ac.js\"],\"default\"]\n8:I[9757,[],\"MetadataBoundary\"]\na:I[9757,[],\"OutletBoundary\"]\nd:I[8475,[],\"AsyncMetadataOutlet\"]\nf:I[9757,[],\"ViewportBoundary\"]\n11:I[8458,[],\"\"]\n:HL[\"/transformer-paper-site/_next/static/css/074657df094ca3ac.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"7PSQnb5Kq0UG_l8rY0OUI\",\"p\":\"/transformer-paper-site\",\"c\":[\"\",\"positional-encoding\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"positional-encoding\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/transformer-paper-site/_next/static/css/074657df094ca3ac.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"zh-CN\",\"children\":[\"$\",\"body\",null,{\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[\"positional-encoding\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"$L4\",null,{\"Component\":\"$5\",\"searchParams\":{},\"params\":{},\"promises\":[\"$@6\",\"$@7\"]}],[\"$\",\"$L8\",null,{\"children\":\"$L9\"}],null,[\"$\",\"$La\",null,{\"children\":[\"$Lb\",\"$Lc\",[\"$\",\"$Ld\",null,{\"promise\":\"$@e\"}]]}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"3k5NGjeVuqqoV7GjfVXmr\",{\"children\":[[\"$\",\"$Lf\",null,{\"children\":\"$L10\"}],null]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$11\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"12:\"$Sreact.suspense\"\n13:I[8475,[],\"AsyncMetadata\"]\n6:{}\n7:{}\n9:[\"$\",\"$12\",null,{\"fallback\":null,\"children\":[\"$\",\"$L13\",null,{\"promise\":\"$@14\"}]}]\n"])</script><script>self.__next_f.push([1,"c:null\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nb:null\n"])</script><script>self.__next_f.push([1,"14:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Transformer论文解析\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"《Attention Is All You Need》论文详细解析，了解Transformer架构的核心创新和工作原理\"}]],\"error\":null,\"digest\":\"$undefined\"}\ne:{\"metadata\":\"$14:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>