(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[974],{1451:(e,r,n)=>{"use strict";n.r(r),n.d(r,{default:()=>c});var i=n(5881),s=n(7210),a=n.n(s),o=n(6797),d=n(8251),t=n(6883);let l=JSON.parse('{"r":"<p>Attention Is All You Need: Transformer架构详解</p>\\n","V":[{"id":"overview","html":"<h2>论文概述</h2>\\n<p>《Attention Is All You Need》是由Google的研究团队在2017年发表的一篇开创性论文，该论文提出了Transformer架构，这一架构完全基于注意力机制，摒弃了之前主流的循环神经网络（RNN）和卷积神经网络（CNN）。Transformer模型通过自注意力机制捕捉序列中的全局依赖关系，显著提高了机器翻译等序列转导任务的性能，并且大大减少了训练时间。</p>\\n<p>该论文发表后，Transformer架构迅速成为自然语言处理领域的基础，后续的BERT、GPT等模型都是在此基础上演变而来，彻底改变了NLP技术的发展方向。</p>\\n"},{"id":"innovations","html":"<h2>关键创新点</h2>\\n<ol>\\n<li><strong>全注意力架构</strong>：Transformer是第一个完全基于注意力机制的序列转导模型，不使用任何循环或卷积层</li>\\n<li><strong>多头自注意力</strong>：通过多个注意力头并行学习不同的表示子空间的信息</li>\\n<li><strong>缩放点积注意力</strong>：改进点积注意力，通过缩放因子防止梯度消失</li>\\n<li><strong>位置编码</strong>：使用正弦和余弦函数生成的位置编码，为模型提供序列中的位置信息</li>\\n<li><strong>并行计算</strong>：不同于RNN的顺序计算，Transformer允许大量并行计算，大大提高了训练效率</li>\\n</ol>\\n"},{"id":"architecture","html":"<h2>Transformer架构详解</h2>\\n<h3>整体架构</h3>\\n<p>Transformer采用经典的编码器-解码器(Encoder-Decoder)结构：</p>\\n<ol>\\n<li><strong>编码器</strong>：由N=6个相同的层堆叠而成</li>\\n<li><strong>解码器</strong>：也由N=6个相同的层堆叠而成</li>\\n</ol>\\n<p>每个编码器层包含两个子层：</p>\\n<ul>\\n<li>多头自注意力层</li>\\n<li>位置前馈网络</li>\\n</ul>\\n<p>每个解码器层包含三个子层：</p>\\n<ul>\\n<li>掩蔽多头自注意力层</li>\\n<li>编码器-解码器注意力层</li>\\n<li>位置前馈网络</li>\\n</ul>\\n<p>所有子层都采用残差连接和层归一化。</p>\\n<h3>核心组件详解</h3>\\n<h4>1. 自注意力机制</h4>\\n<p>自注意力是Transformer的核心创新，它允许模型关注输入序列中的不同位置，计算这些位置之间的依赖关系。</p>\\n<p>具体实现是缩放点积注意力(Scaled Dot-Product Attention)：</p>\\n<pre><code>Attention(Q, K, V) = softmax(QK^T/√d_k)V\\n</code></pre>\\n<p>其中：</p>\\n<ul>\\n<li>Q（查询）、K（键）、V（值）是输入向量的不同线性变换</li>\\n<li>√d_k是缩放因子，防止点积过大导致softmax梯度消失</li>\\n</ul>\\n<h4>2. 多头注意力</h4>\\n<p>多头注意力允许模型同时关注来自不同表示子空间的信息：</p>\\n<pre><code>MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O\\n</code></pre>\\n<p>其中每个head_i是：</p>\\n<pre><code>head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\\n</code></pre>\\n<p>Transformer使用h=8个注意力头，每个头的维度d_k=d_v=d_model/h=64。</p>\\n<h4>3. 位置前馈网络</h4>\\n<p>每个编码器和解码器层包含一个全连接的前馈网络，对每个位置独立应用：</p>\\n<pre><code>FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\\n</code></pre>\\n<p>这是由两个线性变换组成，中间有ReLU激活函数。</p>\\n<h4>4. 位置编码</h4>\\n<p>由于Transformer不使用循环和卷积，它需要额外的位置信息来使用序列的顺序。论文采用正弦和余弦函数的位置编码：</p>\\n<pre><code>PE(pos, 2i) = sin(pos/10000^(2i/d_model))\\nPE(pos, 2i+1) = cos(pos/10000^(2i/d_model))\\n</code></pre>\\n<p>这种编码方式可以让模型学习关注相对位置，因为任何固定偏移k的PE(pos+k)都可以表示为PE(pos)的线性函数。</p>\\n<h3>Transformer的优势</h3>\\n<p>与RNN和CNN相比，Transformer具有以下关键优势：</p>\\n<ol>\\n<li><strong>并行计算</strong>：自注意力允许所有位置同时计算，而RNN必须按顺序计算</li>\\n<li><strong>全局依赖关系</strong>：自注意力直接建立任意两个位置之间的依赖关系，路径长度为O(1)，而RNN为O(n)，CNN为O(log_k(n))</li>\\n<li><strong>更高效的长序列处理</strong>：当序列长度n小于表示维度d时，自注意力的计算复杂度优于RNN</li>\\n</ol>\\n"},{"id":"components","html":"<h2>实验结果</h2>\\n<p>论文在两个机器翻译任务上验证了Transformer的有效性：</p>\\n<ol>\\n<li>\\n<p><strong>WMT 2014英德翻译</strong>：</p>\\n<ul>\\n<li>Transformer(大型模型)达到28.4 BLEU分数</li>\\n<li>超过之前最佳结果2个BLEU以上</li>\\n</ul>\\n</li>\\n<li>\\n<p><strong>WMT 2014英法翻译</strong>：</p>\\n<ul>\\n<li>Transformer(大型模型)达到41.8 BLEU分数</li>\\n<li>仅使用8个GPU训练3.5天，训练成本是之前最佳模型的四分之一</li>\\n</ul>\\n</li>\\n</ol>\\n<p>此外，论文还在英语成分句法分析任务上展示了Transformer的泛化能力。</p>\\n"},{"id":"advantages","html":"<h2>模型变体实验</h2>\\n<p>论文探究了不同组件对模型性能的影响：</p>\\n<ol>\\n<li><strong>注意力头数量</strong>：8个头的效果最佳，过多或过少都会导致性能下降</li>\\n<li><strong>键维度(d_k)</strong>：减小d_k会降低模型质量</li>\\n<li><strong>模型大小</strong>：更大的模型通常表现更好</li>\\n<li><strong>Dropout</strong>：对避免过拟合非常有效</li>\\n<li><strong>位置编码</strong>：学习的位置嵌入与正弦位置编码性能相当</li>\\n</ol>\\n"},{"id":"results","html":"<h2>Transformer的影响与应用</h2>\\n<p>Transformer架构的提出开启了NLP新纪元：</p>\\n<ol>\\n<li>BERT、RoBERTa等预训练语言模型基于Transformer编码器</li>\\n<li>GPT系列基于Transformer解码器</li>\\n<li>将Transformer应用于计算机视觉、音频处理等多模态任务</li>\\n<li>成为目前主流大语言模型的基础架构</li>\\n</ol>\\n"},{"id":"variants","html":"<h2>总结</h2>\\n<p>Transformer架构的提出是自然语言处理领域的一个重大突破，它通过全注意力机制捕捉序列中的全局依赖关系，使模型性能和训练效率都得到了极大提升。这一架构彻底改变了深度学习处理序列数据的方式，并为后续的语言模型发展奠定了基础。</p>\\n"}]}');function c(){let[e,r]=(0,o.useState)([]),[n,s]=(0,o.useState)("");return(0,o.useEffect)(()=>{s(l.r),r(l.V)},[]),(0,i.jsxs)("div",{className:"jsx-afc3c4d5fa6dfd47 layout",children:[(0,i.jsx)(d.A,{}),(0,i.jsx)(t.A,{}),(0,i.jsx)("main",{className:"jsx-afc3c4d5fa6dfd47 main-content",children:(0,i.jsx)("div",{className:"jsx-afc3c4d5fa6dfd47 container",children:(0,i.jsxs)("article",{className:"jsx-afc3c4d5fa6dfd47 paper-content",children:[(0,i.jsx)("section",{className:"jsx-afc3c4d5fa6dfd47 introduction",children:(0,i.jsx)("div",{dangerouslySetInnerHTML:{__html:n},className:"jsx-afc3c4d5fa6dfd47"})}),e.map(e=>(0,i.jsx)("section",{id:e.id,className:"jsx-afc3c4d5fa6dfd47",children:(0,i.jsx)("div",{dangerouslySetInnerHTML:{__html:e.html},className:"jsx-afc3c4d5fa6dfd47"})},e.id))]})})}),(0,i.jsx)(a(),{id:"afc3c4d5fa6dfd47",children:".paper-content.jsx-afc3c4d5fa6dfd47{max-width:800px;margin:0 auto;padding:2rem 0}.paper-content.jsx-afc3c4d5fa6dfd47 section.jsx-afc3c4d5fa6dfd47{margin-bottom:3rem;scroll-margin-top:-webkit-calc(var(--header-height) + 2rem);scroll-margin-top:-moz-calc(var(--header-height) + 2rem);scroll-margin-top:calc(var(--header-height) + 2rem)}@media(max-width:768px){.paper-content.jsx-afc3c4d5fa6dfd47{padding:1rem 0}}"})]})}},6034:(e,r,n)=>{Promise.resolve().then(n.bind(n,1451))},6883:(e,r,n)=>{"use strict";n.d(r,{A:()=>t});var i=n(5881),s=n(7210),a=n.n(s),o=n(70),d=n.n(o);function t(){return(0,i.jsxs)("header",{className:"jsx-dd9b86782d437359 header",children:[(0,i.jsxs)("div",{className:"jsx-dd9b86782d437359 container header-container",children:[(0,i.jsx)("div",{className:"jsx-dd9b86782d437359 logo",children:(0,i.jsx)(d(),{href:"/",children:(0,i.jsx)("span",{className:"jsx-dd9b86782d437359 logo-text",children:"Transformer论文解析"})})}),(0,i.jsxs)("nav",{className:"jsx-dd9b86782d437359 nav",children:[(0,i.jsx)("a",{href:"https://github.com/tensorflow/tensor2tensor",target:"_blank",rel:"noopener noreferrer",className:"jsx-dd9b86782d437359",children:"GitHub源码"}),(0,i.jsx)("a",{href:"https://arxiv.org/abs/1706.03762",target:"_blank",rel:"noopener noreferrer",className:"jsx-dd9b86782d437359",children:"原论文"})]})]}),(0,i.jsx)(a(),{id:"dd9b86782d437359",children:".header.jsx-dd9b86782d437359{position:fixed;top:0;left:0;right:0;height:var(--header-height);background-color:white;-webkit-box-shadow:0 1px 2px rgba(0,0,0,.1);-moz-box-shadow:0 1px 2px rgba(0,0,0,.1);box-shadow:0 1px 2px rgba(0,0,0,.1);z-index:50}.header-container.jsx-dd9b86782d437359{display:-webkit-box;display:-webkit-flex;display:-moz-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-webkit-align-items:center;-moz-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-moz-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;height:100%;padding:0 2rem;margin-left:var(--sidebar-width)}.logo.jsx-dd9b86782d437359{display:-webkit-box;display:-webkit-flex;display:-moz-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-webkit-align-items:center;-moz-box-align:center;-ms-flex-align:center;align-items:center}.logo-text.jsx-dd9b86782d437359{font-size:1.25rem;font-weight:700;color:var(--primary-color)}.nav.jsx-dd9b86782d437359{display:-webkit-box;display:-webkit-flex;display:-moz-box;display:-ms-flexbox;display:flex;gap:1.5rem}.nav.jsx-dd9b86782d437359 a.jsx-dd9b86782d437359{font-weight:500;padding:.5rem 0}@media(max-width:768px){.header-container.jsx-dd9b86782d437359{margin-left:0;padding:0 1rem}.logo-text.jsx-dd9b86782d437359{font-size:1.1rem}.nav.jsx-dd9b86782d437359{gap:1rem}}"})]})}},8251:(e,r,n)=>{"use strict";n.d(r,{A:()=>l});var i=n(5881),s=n(7210),a=n.n(s),o=n(6797),d=n(70),t=n.n(d);function l(){let[e,r]=(0,o.useState)(!1);return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)("button",{onClick:()=>{r(!e)},"aria-label":e?"关闭菜单":"打开菜单",className:"jsx-6dbfbfbce4844938 sidebar-toggle",children:[(0,i.jsx)("span",{className:"jsx-6dbfbfbce4844938"}),(0,i.jsx)("span",{className:"jsx-6dbfbfbce4844938"}),(0,i.jsx)("span",{className:"jsx-6dbfbfbce4844938"})]}),(0,i.jsxs)("aside",{className:"jsx-6dbfbfbce4844938 "+"sidebar ".concat(e?"open":""),children:[(0,i.jsx)("div",{className:"jsx-6dbfbfbce4844938 sidebar-header",children:(0,i.jsx)("h2",{className:"jsx-6dbfbfbce4844938",children:"目录"})}),(0,i.jsx)("nav",{className:"jsx-6dbfbfbce4844938 sidebar-nav",children:(0,i.jsx)("ul",{className:"jsx-6dbfbfbce4844938",children:[{id:"overview",title:"论文概述"},{id:"innovations",title:"关键创新点"},{id:"architecture",title:"Transformer架构详解"},{id:"components",title:"核心组件详解"},{id:"advantages",title:"Transformer的优势"},{id:"results",title:"实验结果"},{id:"variants",title:"模型变体实验"},{id:"impact",title:"Transformer的影响与应用"},{id:"conclusion",title:"总结"}].map(e=>(0,i.jsx)("li",{className:"jsx-6dbfbfbce4844938",children:(0,i.jsx)(t(),{href:"#".concat(e.id),onClick:()=>r(!1),children:e.title})},e.id))})}),(0,i.jsx)("div",{className:"jsx-6dbfbfbce4844938 sidebar-footer",children:(0,i.jsx)("a",{href:"https://arxiv.org/abs/1706.03762",target:"_blank",rel:"noopener noreferrer",className:"jsx-6dbfbfbce4844938",children:"原论文链接"})})]}),(0,i.jsx)(a(),{id:"6dbfbfbce4844938",children:".sidebar.jsx-6dbfbfbce4844938{position:fixed;top:0;left:0;width:var(--sidebar-width);height:100vh;background-color:white;border-right:1px solid var(--border-color);padding:1.5rem;overflow-y:auto;z-index:100;-webkit-transition:-webkit-transform.3s ease;-moz-transition:-moz-transform.3s ease;-o-transition:-o-transform.3s ease;transition:-webkit-transform.3s ease;transition:-moz-transform.3s ease;transition:-o-transform.3s ease;transition:transform.3s ease}.sidebar-header.jsx-6dbfbfbce4844938{margin-bottom:1.5rem;padding-bottom:1rem;border-bottom:1px solid var(--border-color)}.sidebar-header.jsx-6dbfbfbce4844938 h2.jsx-6dbfbfbce4844938{margin:0;font-size:1.5rem;color:var(--primary-color)}.sidebar-nav.jsx-6dbfbfbce4844938 ul.jsx-6dbfbfbce4844938{list-style:none;padding:0}.sidebar-nav.jsx-6dbfbfbce4844938 li.jsx-6dbfbfbce4844938{margin-bottom:.8rem}.sidebar-nav.jsx-6dbfbfbce4844938 a.jsx-6dbfbfbce4844938{display:block;padding:.5rem 0;color:var(--text-color)}.sidebar-nav.jsx-6dbfbfbce4844938 a.jsx-6dbfbfbce4844938:hover{color:var(--primary-color)}.sidebar-footer.jsx-6dbfbfbce4844938{margin-top:2rem;padding-top:1rem;border-top:1px solid var(--border-color)}.sidebar-footer.jsx-6dbfbfbce4844938 a.jsx-6dbfbfbce4844938{display:block;padding:.5rem 0;color:var(--primary-color);font-weight:500}.sidebar-toggle.jsx-6dbfbfbce4844938{position:fixed;top:1rem;left:1rem;z-index:101;display:none;-webkit-box-orient:vertical;-webkit-box-direction:normal;-webkit-flex-direction:column;-moz-box-orient:vertical;-moz-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-justify-content:space-around;-ms-flex-pack:distribute;justify-content:space-around;width:2rem;height:2rem;background:transparent;border:none;cursor:pointer;padding:0}.sidebar-toggle.jsx-6dbfbfbce4844938 span.jsx-6dbfbfbce4844938{width:2rem;height:.25rem;background:var(--primary-color);-webkit-border-radius:10px;-moz-border-radius:10px;border-radius:10px;-webkit-transition:all.3s linear;-moz-transition:all.3s linear;-o-transition:all.3s linear;transition:all.3s linear;position:relative;-webkit-transform-origin:1px;-moz-transform-origin:1px;-ms-transform-origin:1px;-o-transform-origin:1px;transform-origin:1px}@media(max-width:768px){.sidebar-toggle.jsx-6dbfbfbce4844938{display:-webkit-box;display:-webkit-flex;display:-moz-box;display:-ms-flexbox;display:flex}}"})]})}}},e=>{var r=r=>e(e.s=r);e.O(0,[91,364,770,358],()=>r(6034)),_N_E=e.O()}]);