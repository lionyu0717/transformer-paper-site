(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[306],{2289:(e,s,a)=>{"use strict";a.d(s,{A:()=>o});var r=a(5881),i=a(7210),n=a.n(i);function o(e){let{code:s,language:a}=e;return(0,r.jsxs)("div",{className:"jsx-f15f37c580e7221a code-block",children:[(0,r.jsx)("pre",{className:"jsx-f15f37c580e7221a",children:(0,r.jsx)("code",{className:"jsx-f15f37c580e7221a "+((a?"language-".concat(a):"")||""),children:s})}),(0,r.jsx)(n(),{id:"f15f37c580e7221a",children:'.code-block.jsx-f15f37c580e7221a{margin:1.5rem 0;-webkit-border-radius:6px;-moz-border-radius:6px;border-radius:6px;overflow:hidden}pre.jsx-f15f37c580e7221a{background-color:#1e293b;color:#e2e8f0;padding:1.25rem;overflow-x:auto;font-size:.9rem;line-height:1.6;margin:0}code.jsx-f15f37c580e7221a{font-family:"SFMono-Regular",Consolas,"Liberation Mono",Menlo,monospace;background:transparent;padding:0}'})]})}a(6797)},5053:(e,s,a)=>{"use strict";a.r(s),a.d(s,{default:()=>x});var r=a(5881),i=a(7210),n=a.n(i),o=a(8251),d=a(6883),c=a(1309),t=a(70),l=a.n(t),b=a(2289);function x(){return(0,r.jsxs)("div",{className:"jsx-57fc471b20120289 layout",children:[(0,r.jsx)(o.A,{}),(0,r.jsx)(d.A,{}),(0,r.jsx)("main",{className:"jsx-57fc471b20120289 main-content",children:(0,r.jsx)("div",{className:"jsx-57fc471b20120289 container",children:(0,r.jsxs)("article",{className:"jsx-57fc471b20120289 attention-content",children:[(0,r.jsx)("h1",{className:"jsx-57fc471b20120289",children:"自注意力机制详解"}),(0,r.jsxs)("section",{className:"jsx-57fc471b20120289",children:[(0,r.jsx)("h2",{className:"jsx-57fc471b20120289",children:"自注意力的基本原理"}),(0,r.jsx)("p",{className:"jsx-57fc471b20120289",children:"自注意力是Transformer的核心创新，它允许模型在处理序列时关注到序列中的不同位置， 计算这些位置之间的依赖关系。与RNN不同，自注意力可以直接建立任意两个位置之间的连接， 因此能够更有效地捕捉长距离依赖。"}),(0,r.jsxs)("div",{className:"jsx-57fc471b20120289 image-container",children:[(0,r.jsx)(c.default,{src:"/self-attention.svg",alt:"自注意力机制示意图",width:700,height:400,priority:!0}),(0,r.jsx)("p",{className:"jsx-57fc471b20120289 image-caption",children:"图1：自注意力机制示意图"})]})]}),(0,r.jsxs)("section",{className:"jsx-57fc471b20120289",children:[(0,r.jsx)("h2",{className:"jsx-57fc471b20120289",children:"缩放点积注意力"}),(0,r.jsx)("p",{className:"jsx-57fc471b20120289",children:"Transformer使用了缩放点积注意力(Scaled Dot-Product Attention)，其计算公式如下："}),(0,r.jsx)(b.A,{code:"Attention(Q, K, V) = softmax(QK^T/√d_k)V",language:"python"}),(0,r.jsx)("p",{className:"jsx-57fc471b20120289",children:"其中："}),(0,r.jsxs)("ul",{className:"jsx-57fc471b20120289",children:[(0,r.jsxs)("li",{className:"jsx-57fc471b20120289",children:[(0,r.jsx)("strong",{className:"jsx-57fc471b20120289",children:"Q (查询)"}),"：由输入序列通过线性投影层生成的查询矩阵"]}),(0,r.jsxs)("li",{className:"jsx-57fc471b20120289",children:[(0,r.jsx)("strong",{className:"jsx-57fc471b20120289",children:"K (键)"}),"：由输入序列通过线性投影层生成的键矩阵"]}),(0,r.jsxs)("li",{className:"jsx-57fc471b20120289",children:[(0,r.jsx)("strong",{className:"jsx-57fc471b20120289",children:"V (值)"}),"：由输入序列通过线性投影层生成的值矩阵"]}),(0,r.jsxs)("li",{className:"jsx-57fc471b20120289",children:[(0,r.jsx)("strong",{className:"jsx-57fc471b20120289",children:"√d_k"}),"：缩放因子，d_k是键向量的维度，这个缩放操作防止点积过大导致softmax梯度消失"]})]}),(0,r.jsx)("h3",{className:"jsx-57fc471b20120289",children:"计算步骤详解："}),(0,r.jsxs)("ol",{className:"jsx-57fc471b20120289",children:[(0,r.jsx)("li",{className:"jsx-57fc471b20120289",children:"计算查询(Q)和键(K)的点积，得到注意力分数矩阵"}),(0,r.jsx)("li",{className:"jsx-57fc471b20120289",children:"将注意力分数除以√d_k进行缩放"}),(0,r.jsx)("li",{className:"jsx-57fc471b20120289",children:"对缩放后的注意力分数应用softmax，得到注意力权重"}),(0,r.jsx)("li",{className:"jsx-57fc471b20120289",children:"将注意力权重与值(V)矩阵相乘，得到加权求和的输出"})]})]}),(0,r.jsxs)("section",{className:"jsx-57fc471b20120289",children:[(0,r.jsx)("h2",{className:"jsx-57fc471b20120289",children:"多头注意力机制"}),(0,r.jsx)("p",{className:"jsx-57fc471b20120289",children:"多头注意力(Multi-Head Attention)允许模型同时关注来自不同表示子空间的信息，从而提高模型的表达能力。 其计算公式如下："}),(0,r.jsx)(b.A,{code:"MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O\n\nhead_i = Attention(QW_i^Q, KW_i^K, VW_i^V)",language:"python"}),(0,r.jsx)("p",{className:"jsx-57fc471b20120289",children:"Transformer使用h=8个注意力头，即同时计算8个不同的自注意力，然后将结果拼接并投影到原始维度。 每个头的维度d_k = d_v = d_model/h = 64。"}),(0,r.jsx)("p",{className:"jsx-57fc471b20120289",children:"多头注意力的优势："}),(0,r.jsxs)("ul",{className:"jsx-57fc471b20120289",children:[(0,r.jsx)("li",{className:"jsx-57fc471b20120289",children:"允许模型关注序列的不同方面"}),(0,r.jsx)("li",{className:"jsx-57fc471b20120289",children:"增强模型的表达能力和特征提取能力"}),(0,r.jsx)("li",{className:"jsx-57fc471b20120289",children:"提高模型的稳定性和泛化能力"})]})]}),(0,r.jsxs)("section",{className:"jsx-57fc471b20120289",children:[(0,r.jsx)("h2",{className:"jsx-57fc471b20120289",children:"注意力的直观理解"}),(0,r.jsx)("p",{className:"jsx-57fc471b20120289",children:'可以将自注意力理解为对输入序列中每个元素分配一个"重要性权重"， 这个权重决定了该元素对当前位置的影响程度。'}),(0,r.jsx)("p",{className:"jsx-57fc471b20120289",children:'例如，在翻译任务中，解码器生成某个目标单词时，通过注意力机制可以"关注"源句中最相关的单词， 从而实现更准确的翻译。与传统的RNN相比，这种直接建立的长距离依赖使得模型能更好地处理长句子。'})]}),(0,r.jsxs)("div",{className:"jsx-57fc471b20120289 navigation-links",children:[(0,r.jsx)(l(),{href:"/",className:"nav-link",children:"返回首页"}),(0,r.jsx)(l(),{href:"/architecture",className:"nav-link",children:"架构详解"})]})]})})}),(0,r.jsx)(n(),{id:"57fc471b20120289",children:".attention-content.jsx-57fc471b20120289{max-width:800px;margin:0 auto;padding:2rem 0}h1.jsx-57fc471b20120289{margin-bottom:2rem;color:var(--primary-color)}section.jsx-57fc471b20120289{margin-bottom:3rem}.image-container.jsx-57fc471b20120289{margin:2rem 0;text-align:center}.image-caption.jsx-57fc471b20120289{margin-top:1rem;font-size:.9rem;color:#64748b;text-align:center}.navigation-links.jsx-57fc471b20120289{margin-top:3rem;padding-top:1.5rem;border-top:1px solid var(--border-color);display:-webkit-box;display:-webkit-flex;display:-moz-box;display:-ms-flexbox;display:flex;-webkit-justify-content:space-around;-ms-flex-pack:distribute;justify-content:space-around}.nav-link.jsx-57fc471b20120289{display:inline-block;padding:.75rem 1.5rem;background-color:var(--primary-color);color:white;-webkit-border-radius:6px;-moz-border-radius:6px;border-radius:6px;font-weight:500;-webkit-transition:background-color.2s;-moz-transition:background-color.2s;-o-transition:background-color.2s;transition:background-color.2s}.nav-link.jsx-57fc471b20120289:hover{background-color:var(--link-hover-color);color:white}"})]})}},6883:(e,s,a)=>{"use strict";a.d(s,{A:()=>c});var r=a(5881),i=a(7210),n=a.n(i),o=a(70),d=a.n(o);function c(){return(0,r.jsxs)("header",{className:"jsx-dd9b86782d437359 header",children:[(0,r.jsxs)("div",{className:"jsx-dd9b86782d437359 container header-container",children:[(0,r.jsx)("div",{className:"jsx-dd9b86782d437359 logo",children:(0,r.jsx)(d(),{href:"/",children:(0,r.jsx)("span",{className:"jsx-dd9b86782d437359 logo-text",children:"Transformer论文解析"})})}),(0,r.jsxs)("nav",{className:"jsx-dd9b86782d437359 nav",children:[(0,r.jsx)("a",{href:"https://github.com/tensorflow/tensor2tensor",target:"_blank",rel:"noopener noreferrer",className:"jsx-dd9b86782d437359",children:"GitHub源码"}),(0,r.jsx)("a",{href:"https://arxiv.org/abs/1706.03762",target:"_blank",rel:"noopener noreferrer",className:"jsx-dd9b86782d437359",children:"原论文"})]})]}),(0,r.jsx)(n(),{id:"dd9b86782d437359",children:".header.jsx-dd9b86782d437359{position:fixed;top:0;left:0;right:0;height:var(--header-height);background-color:white;-webkit-box-shadow:0 1px 2px rgba(0,0,0,.1);-moz-box-shadow:0 1px 2px rgba(0,0,0,.1);box-shadow:0 1px 2px rgba(0,0,0,.1);z-index:50}.header-container.jsx-dd9b86782d437359{display:-webkit-box;display:-webkit-flex;display:-moz-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-webkit-align-items:center;-moz-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-moz-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;height:100%;padding:0 2rem;margin-left:var(--sidebar-width)}.logo.jsx-dd9b86782d437359{display:-webkit-box;display:-webkit-flex;display:-moz-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-webkit-align-items:center;-moz-box-align:center;-ms-flex-align:center;align-items:center}.logo-text.jsx-dd9b86782d437359{font-size:1.25rem;font-weight:700;color:var(--primary-color)}.nav.jsx-dd9b86782d437359{display:-webkit-box;display:-webkit-flex;display:-moz-box;display:-ms-flexbox;display:flex;gap:1.5rem}.nav.jsx-dd9b86782d437359 a.jsx-dd9b86782d437359{font-weight:500;padding:.5rem 0}@media(max-width:768px){.header-container.jsx-dd9b86782d437359{margin-left:0;padding:0 1rem}.logo-text.jsx-dd9b86782d437359{font-size:1.1rem}.nav.jsx-dd9b86782d437359{gap:1rem}}"})]})}},8251:(e,s,a)=>{"use strict";a.d(s,{A:()=>t});var r=a(5881),i=a(7210),n=a.n(i),o=a(6797),d=a(70),c=a.n(d);function t(){let[e,s]=(0,o.useState)(!1);return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)("button",{onClick:()=>{s(!e)},"aria-label":e?"关闭菜单":"打开菜单",className:"jsx-6dbfbfbce4844938 sidebar-toggle",children:[(0,r.jsx)("span",{className:"jsx-6dbfbfbce4844938"}),(0,r.jsx)("span",{className:"jsx-6dbfbfbce4844938"}),(0,r.jsx)("span",{className:"jsx-6dbfbfbce4844938"})]}),(0,r.jsxs)("aside",{className:"jsx-6dbfbfbce4844938 "+"sidebar ".concat(e?"open":""),children:[(0,r.jsx)("div",{className:"jsx-6dbfbfbce4844938 sidebar-header",children:(0,r.jsx)("h2",{className:"jsx-6dbfbfbce4844938",children:"目录"})}),(0,r.jsx)("nav",{className:"jsx-6dbfbfbce4844938 sidebar-nav",children:(0,r.jsx)("ul",{className:"jsx-6dbfbfbce4844938",children:[{id:"overview",title:"论文概述"},{id:"innovations",title:"关键创新点"},{id:"architecture",title:"Transformer架构详解"},{id:"components",title:"核心组件详解"},{id:"advantages",title:"Transformer的优势"},{id:"results",title:"实验结果"},{id:"variants",title:"模型变体实验"},{id:"impact",title:"Transformer的影响与应用"},{id:"conclusion",title:"总结"}].map(e=>(0,r.jsx)("li",{className:"jsx-6dbfbfbce4844938",children:(0,r.jsx)(c(),{href:"#".concat(e.id),onClick:()=>s(!1),children:e.title})},e.id))})}),(0,r.jsx)("div",{className:"jsx-6dbfbfbce4844938 sidebar-footer",children:(0,r.jsx)("a",{href:"https://arxiv.org/abs/1706.03762",target:"_blank",rel:"noopener noreferrer",className:"jsx-6dbfbfbce4844938",children:"原论文链接"})})]}),(0,r.jsx)(n(),{id:"6dbfbfbce4844938",children:".sidebar.jsx-6dbfbfbce4844938{position:fixed;top:0;left:0;width:var(--sidebar-width);height:100vh;background-color:white;border-right:1px solid var(--border-color);padding:1.5rem;overflow-y:auto;z-index:100;-webkit-transition:-webkit-transform.3s ease;-moz-transition:-moz-transform.3s ease;-o-transition:-o-transform.3s ease;transition:-webkit-transform.3s ease;transition:-moz-transform.3s ease;transition:-o-transform.3s ease;transition:transform.3s ease}.sidebar-header.jsx-6dbfbfbce4844938{margin-bottom:1.5rem;padding-bottom:1rem;border-bottom:1px solid var(--border-color)}.sidebar-header.jsx-6dbfbfbce4844938 h2.jsx-6dbfbfbce4844938{margin:0;font-size:1.5rem;color:var(--primary-color)}.sidebar-nav.jsx-6dbfbfbce4844938 ul.jsx-6dbfbfbce4844938{list-style:none;padding:0}.sidebar-nav.jsx-6dbfbfbce4844938 li.jsx-6dbfbfbce4844938{margin-bottom:.8rem}.sidebar-nav.jsx-6dbfbfbce4844938 a.jsx-6dbfbfbce4844938{display:block;padding:.5rem 0;color:var(--text-color)}.sidebar-nav.jsx-6dbfbfbce4844938 a.jsx-6dbfbfbce4844938:hover{color:var(--primary-color)}.sidebar-footer.jsx-6dbfbfbce4844938{margin-top:2rem;padding-top:1rem;border-top:1px solid var(--border-color)}.sidebar-footer.jsx-6dbfbfbce4844938 a.jsx-6dbfbfbce4844938{display:block;padding:.5rem 0;color:var(--primary-color);font-weight:500}.sidebar-toggle.jsx-6dbfbfbce4844938{position:fixed;top:1rem;left:1rem;z-index:101;display:none;-webkit-box-orient:vertical;-webkit-box-direction:normal;-webkit-flex-direction:column;-moz-box-orient:vertical;-moz-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-justify-content:space-around;-ms-flex-pack:distribute;justify-content:space-around;width:2rem;height:2rem;background:transparent;border:none;cursor:pointer;padding:0}.sidebar-toggle.jsx-6dbfbfbce4844938 span.jsx-6dbfbfbce4844938{width:2rem;height:.25rem;background:var(--primary-color);-webkit-border-radius:10px;-moz-border-radius:10px;border-radius:10px;-webkit-transition:all.3s linear;-moz-transition:all.3s linear;-o-transition:all.3s linear;transition:all.3s linear;position:relative;-webkit-transform-origin:1px;-moz-transform-origin:1px;-ms-transform-origin:1px;-o-transform-origin:1px;transform-origin:1px}@media(max-width:768px){.sidebar-toggle.jsx-6dbfbfbce4844938{display:-webkit-box;display:-webkit-flex;display:-moz-box;display:-ms-flexbox;display:flex}}"})]})}},9380:(e,s,a)=>{Promise.resolve().then(a.bind(a,5053))}},e=>{var s=s=>e(e.s=s);e.O(0,[91,690,364,770,358],()=>s(9380)),_N_E=e.O()}]);