<!DOCTYPE html><html lang="zh-CN"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/self-attention.svg"/><link rel="stylesheet" href="/transformer-paper-site/_next/static/css/074657df094ca3ac.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/transformer-paper-site/_next/static/chunks/webpack-1c2f0ab7e589b28f.js"/><script src="/transformer-paper-site/_next/static/chunks/96e220d1-dd08d031967947a2.js" async=""></script><script src="/transformer-paper-site/_next/static/chunks/770-131adaed12890c69.js" async=""></script><script src="/transformer-paper-site/_next/static/chunks/main-app-440889e68b6317e3.js" async=""></script><script src="/transformer-paper-site/_next/static/chunks/91-e2c009cf22ee4bab.js" async=""></script><script src="/transformer-paper-site/_next/static/chunks/690-1ea658c9bde215d9.js" async=""></script><script src="/transformer-paper-site/_next/static/chunks/app/self-attention/page-7edb1ea56c667add.js" async=""></script><title>Transformer论文解析</title><meta name="description" content="《Attention Is All You Need》论文详细解析，了解Transformer架构的核心创新和工作原理"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/transformer-paper-site/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body><div class="jsx-57fc471b20120289 layout"><button aria-label="打开菜单" class="jsx-6dbfbfbce4844938 sidebar-toggle"><span class="jsx-6dbfbfbce4844938"></span><span class="jsx-6dbfbfbce4844938"></span><span class="jsx-6dbfbfbce4844938"></span></button><aside class="jsx-6dbfbfbce4844938 sidebar "><div class="jsx-6dbfbfbce4844938 sidebar-header"><h2 class="jsx-6dbfbfbce4844938">目录</h2></div><nav class="jsx-6dbfbfbce4844938 sidebar-nav"><ul class="jsx-6dbfbfbce4844938"><li class="jsx-6dbfbfbce4844938"><a href="#overview">论文概述</a></li><li class="jsx-6dbfbfbce4844938"><a href="#innovations">关键创新点</a></li><li class="jsx-6dbfbfbce4844938"><a href="#architecture">Transformer架构详解</a></li><li class="jsx-6dbfbfbce4844938"><a href="#components">核心组件详解</a></li><li class="jsx-6dbfbfbce4844938"><a href="#advantages">Transformer的优势</a></li><li class="jsx-6dbfbfbce4844938"><a href="#results">实验结果</a></li><li class="jsx-6dbfbfbce4844938"><a href="#variants">模型变体实验</a></li><li class="jsx-6dbfbfbce4844938"><a href="#impact">Transformer的影响与应用</a></li><li class="jsx-6dbfbfbce4844938"><a href="#conclusion">总结</a></li></ul></nav><div class="jsx-6dbfbfbce4844938 sidebar-footer"><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer" class="jsx-6dbfbfbce4844938">原论文链接</a></div></aside><header class="jsx-dd9b86782d437359 header"><div class="jsx-dd9b86782d437359 container header-container"><div class="jsx-dd9b86782d437359 logo"><a href="/transformer-paper-site/"><span class="jsx-dd9b86782d437359 logo-text">Transformer论文解析</span></a></div><nav class="jsx-dd9b86782d437359 nav"><a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener noreferrer" class="jsx-dd9b86782d437359">GitHub源码</a><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer" class="jsx-dd9b86782d437359">原论文</a></nav></div></header><main class="jsx-57fc471b20120289 main-content"><div class="jsx-57fc471b20120289 container"><article class="jsx-57fc471b20120289 attention-content"><h1 class="jsx-57fc471b20120289">自注意力机制详解</h1><section class="jsx-57fc471b20120289"><h2 class="jsx-57fc471b20120289">自注意力的基本原理</h2><p class="jsx-57fc471b20120289">自注意力是Transformer的核心创新，它允许模型在处理序列时关注到序列中的不同位置， 计算这些位置之间的依赖关系。与RNN不同，自注意力可以直接建立任意两个位置之间的连接， 因此能够更有效地捕捉长距离依赖。</p><div class="jsx-57fc471b20120289 image-container"><img alt="自注意力机制示意图" width="700" height="400" decoding="async" data-nimg="1" style="color:transparent" src="/self-attention.svg"/><p class="jsx-57fc471b20120289 image-caption">图1：自注意力机制示意图</p></div></section><section class="jsx-57fc471b20120289"><h2 class="jsx-57fc471b20120289">缩放点积注意力</h2><p class="jsx-57fc471b20120289">Transformer使用了缩放点积注意力(Scaled Dot-Product Attention)，其计算公式如下：</p><div class="jsx-f15f37c580e7221a code-block"><pre class="jsx-f15f37c580e7221a"><code class="jsx-f15f37c580e7221a language-python">Attention(Q, K, V) = softmax(QK^T/√d_k)V</code></pre></div><p class="jsx-57fc471b20120289">其中：</p><ul class="jsx-57fc471b20120289"><li class="jsx-57fc471b20120289"><strong class="jsx-57fc471b20120289">Q (查询)</strong>：由输入序列通过线性投影层生成的查询矩阵</li><li class="jsx-57fc471b20120289"><strong class="jsx-57fc471b20120289">K (键)</strong>：由输入序列通过线性投影层生成的键矩阵</li><li class="jsx-57fc471b20120289"><strong class="jsx-57fc471b20120289">V (值)</strong>：由输入序列通过线性投影层生成的值矩阵</li><li class="jsx-57fc471b20120289"><strong class="jsx-57fc471b20120289">√d_k</strong>：缩放因子，d_k是键向量的维度，这个缩放操作防止点积过大导致softmax梯度消失</li></ul><h3 class="jsx-57fc471b20120289">计算步骤详解：</h3><ol class="jsx-57fc471b20120289"><li class="jsx-57fc471b20120289">计算查询(Q)和键(K)的点积，得到注意力分数矩阵</li><li class="jsx-57fc471b20120289">将注意力分数除以√d_k进行缩放</li><li class="jsx-57fc471b20120289">对缩放后的注意力分数应用softmax，得到注意力权重</li><li class="jsx-57fc471b20120289">将注意力权重与值(V)矩阵相乘，得到加权求和的输出</li></ol></section><section class="jsx-57fc471b20120289"><h2 class="jsx-57fc471b20120289">多头注意力机制</h2><p class="jsx-57fc471b20120289">多头注意力(Multi-Head Attention)允许模型同时关注来自不同表示子空间的信息，从而提高模型的表达能力。 其计算公式如下：</p><div class="jsx-f15f37c580e7221a code-block"><pre class="jsx-f15f37c580e7221a"><code class="jsx-f15f37c580e7221a language-python">MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O

head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)</code></pre></div><p class="jsx-57fc471b20120289">Transformer使用h=8个注意力头，即同时计算8个不同的自注意力，然后将结果拼接并投影到原始维度。 每个头的维度d_k = d_v = d_model/h = 64。</p><p class="jsx-57fc471b20120289">多头注意力的优势：</p><ul class="jsx-57fc471b20120289"><li class="jsx-57fc471b20120289">允许模型关注序列的不同方面</li><li class="jsx-57fc471b20120289">增强模型的表达能力和特征提取能力</li><li class="jsx-57fc471b20120289">提高模型的稳定性和泛化能力</li></ul></section><section class="jsx-57fc471b20120289"><h2 class="jsx-57fc471b20120289">注意力的直观理解</h2><p class="jsx-57fc471b20120289">可以将自注意力理解为对输入序列中每个元素分配一个&quot;重要性权重&quot;， 这个权重决定了该元素对当前位置的影响程度。</p><p class="jsx-57fc471b20120289">例如，在翻译任务中，解码器生成某个目标单词时，通过注意力机制可以&quot;关注&quot;源句中最相关的单词， 从而实现更准确的翻译。与传统的RNN相比，这种直接建立的长距离依赖使得模型能更好地处理长句子。</p></section><div class="jsx-57fc471b20120289 navigation-links"><a class="nav-link" href="/transformer-paper-site/">返回首页</a><a class="nav-link" href="/transformer-paper-site/architecture/">架构详解</a></div></article></div></main></div><!--$--><!--/$--><!--$--><!--/$--><script src="/transformer-paper-site/_next/static/chunks/webpack-1c2f0ab7e589b28f.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[831,[],\"\"]\n3:I[8307,[],\"\"]\n4:I[9226,[],\"ClientPageRoot\"]\n5:I[5053,[\"91\",\"static/chunks/91-e2c009cf22ee4bab.js\",\"690\",\"static/chunks/690-1ea658c9bde215d9.js\",\"306\",\"static/chunks/app/self-attention/page-7edb1ea56c667add.js\"],\"default\"]\n8:I[9757,[],\"MetadataBoundary\"]\na:I[9757,[],\"OutletBoundary\"]\nd:I[8475,[],\"AsyncMetadataOutlet\"]\nf:I[9757,[],\"ViewportBoundary\"]\n11:I[8458,[],\"\"]\n:HL[\"/transformer-paper-site/_next/static/css/074657df094ca3ac.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"7PSQnb5Kq0UG_l8rY0OUI\",\"p\":\"/transformer-paper-site\",\"c\":[\"\",\"self-attention\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"self-attention\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/transformer-paper-site/_next/static/css/074657df094ca3ac.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"zh-CN\",\"children\":[\"$\",\"body\",null,{\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[\"self-attention\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"$L4\",null,{\"Component\":\"$5\",\"searchParams\":{},\"params\":{},\"promises\":[\"$@6\",\"$@7\"]}],[\"$\",\"$L8\",null,{\"children\":\"$L9\"}],null,[\"$\",\"$La\",null,{\"children\":[\"$Lb\",\"$Lc\",[\"$\",\"$Ld\",null,{\"promise\":\"$@e\"}]]}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"mNCaj1p7s_bnUXJN0QyFo\",{\"children\":[[\"$\",\"$Lf\",null,{\"children\":\"$L10\"}],null]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$11\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"12:\"$Sreact.suspense\"\n13:I[8475,[],\"AsyncMetadata\"]\n6:{}\n7:{}\n9:[\"$\",\"$12\",null,{\"fallback\":null,\"children\":[\"$\",\"$L13\",null,{\"promise\":\"$@14\"}]}]\n"])</script><script>self.__next_f.push([1,"c:null\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nb:null\n"])</script><script>self.__next_f.push([1,"14:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Transformer论文解析\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"《Attention Is All You Need》论文详细解析，了解Transformer架构的核心创新和工作原理\"}]],\"error\":null,\"digest\":\"$undefined\"}\ne:{\"metadata\":\"$14:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>