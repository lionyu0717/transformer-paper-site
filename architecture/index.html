<!DOCTYPE html><html lang="zh-CN"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/transformer-architecture.svg"/><link rel="stylesheet" href="/transformer-paper-site/_next/static/css/074657df094ca3ac.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/transformer-paper-site/_next/static/chunks/webpack-1c2f0ab7e589b28f.js"/><script src="/transformer-paper-site/_next/static/chunks/96e220d1-dd08d031967947a2.js" async=""></script><script src="/transformer-paper-site/_next/static/chunks/770-131adaed12890c69.js" async=""></script><script src="/transformer-paper-site/_next/static/chunks/main-app-440889e68b6317e3.js" async=""></script><script src="/transformer-paper-site/_next/static/chunks/91-e2c009cf22ee4bab.js" async=""></script><script src="/transformer-paper-site/_next/static/chunks/690-1ea658c9bde215d9.js" async=""></script><script src="/transformer-paper-site/_next/static/chunks/app/architecture/page-c0294095c88c3613.js" async=""></script><title>Transformer论文解析</title><meta name="description" content="《Attention Is All You Need》论文详细解析，了解Transformer架构的核心创新和工作原理"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/transformer-paper-site/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body><div class="jsx-eb394eaa8dcc9e37 layout"><button aria-label="打开菜单" class="jsx-6dbfbfbce4844938 sidebar-toggle"><span class="jsx-6dbfbfbce4844938"></span><span class="jsx-6dbfbfbce4844938"></span><span class="jsx-6dbfbfbce4844938"></span></button><aside class="jsx-6dbfbfbce4844938 sidebar "><div class="jsx-6dbfbfbce4844938 sidebar-header"><h2 class="jsx-6dbfbfbce4844938">目录</h2></div><nav class="jsx-6dbfbfbce4844938 sidebar-nav"><ul class="jsx-6dbfbfbce4844938"><li class="jsx-6dbfbfbce4844938"><a href="#overview">论文概述</a></li><li class="jsx-6dbfbfbce4844938"><a href="#innovations">关键创新点</a></li><li class="jsx-6dbfbfbce4844938"><a href="#architecture">Transformer架构详解</a></li><li class="jsx-6dbfbfbce4844938"><a href="#components">核心组件详解</a></li><li class="jsx-6dbfbfbce4844938"><a href="#advantages">Transformer的优势</a></li><li class="jsx-6dbfbfbce4844938"><a href="#results">实验结果</a></li><li class="jsx-6dbfbfbce4844938"><a href="#variants">模型变体实验</a></li><li class="jsx-6dbfbfbce4844938"><a href="#impact">Transformer的影响与应用</a></li><li class="jsx-6dbfbfbce4844938"><a href="#conclusion">总结</a></li></ul></nav><div class="jsx-6dbfbfbce4844938 sidebar-footer"><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer" class="jsx-6dbfbfbce4844938">原论文链接</a></div></aside><header class="jsx-dd9b86782d437359 header"><div class="jsx-dd9b86782d437359 container header-container"><div class="jsx-dd9b86782d437359 logo"><a href="/transformer-paper-site/"><span class="jsx-dd9b86782d437359 logo-text">Transformer论文解析</span></a></div><nav class="jsx-dd9b86782d437359 nav"><a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener noreferrer" class="jsx-dd9b86782d437359">GitHub源码</a><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer" class="jsx-dd9b86782d437359">原论文</a></nav></div></header><main class="jsx-eb394eaa8dcc9e37 main-content"><div class="jsx-eb394eaa8dcc9e37 container"><article class="jsx-eb394eaa8dcc9e37 architecture-content"><h1 class="jsx-eb394eaa8dcc9e37">Transformer架构详解</h1><section class="jsx-eb394eaa8dcc9e37 architecture-overview"><h2 class="jsx-eb394eaa8dcc9e37">模型整体架构</h2><p class="jsx-eb394eaa8dcc9e37">Transformer采用经典的编码器-解码器架构，但完全基于注意力机制，摒弃了之前主流的RNN和CNN结构。 下图展示了Transformer的整体架构，包括左侧的编码器和右侧的解码器。</p><div class="jsx-eb394eaa8dcc9e37 architecture-image"><img alt="Transformer架构图" width="800" height="600" decoding="async" data-nimg="1" style="color:transparent" src="/transformer-architecture.svg"/><p class="jsx-eb394eaa8dcc9e37 image-caption">图1：Transformer模型架构</p></div><p class="jsx-eb394eaa8dcc9e37">Transformer的核心特点：</p><ul class="jsx-eb394eaa8dcc9e37"><li class="jsx-eb394eaa8dcc9e37"><strong class="jsx-eb394eaa8dcc9e37">编码器(Encoder)</strong>：由N=6个相同的层堆叠而成</li><li class="jsx-eb394eaa8dcc9e37"><strong class="jsx-eb394eaa8dcc9e37">解码器(Decoder)</strong>：也由N=6个相同的层堆叠而成</li><li class="jsx-eb394eaa8dcc9e37"><strong class="jsx-eb394eaa8dcc9e37">自注意力机制</strong>：允许模型在不增加计算复杂度的情况下捕捉长距离依赖</li><li class="jsx-eb394eaa8dcc9e37"><strong class="jsx-eb394eaa8dcc9e37">多头注意力</strong>：并行关注不同表示子空间的信息</li><li class="jsx-eb394eaa8dcc9e37"><strong class="jsx-eb394eaa8dcc9e37">位置编码</strong>：由于缺少循环结构，使用位置编码提供序列位置信息</li></ul></section><section class="jsx-eb394eaa8dcc9e37 encoder-section"><h2 class="jsx-eb394eaa8dcc9e37">编码器详解</h2><p class="jsx-eb394eaa8dcc9e37">每个编码器层包含两个子层：</p><ol class="jsx-eb394eaa8dcc9e37"><li class="jsx-eb394eaa8dcc9e37"><strong class="jsx-eb394eaa8dcc9e37">多头自注意力层</strong>：允许编码器关注输入序列中的不同位置</li><li class="jsx-eb394eaa8dcc9e37"><strong class="jsx-eb394eaa8dcc9e37">前馈神经网络</strong>：包含两个线性变换和一个ReLU激活函数</li></ol><p class="jsx-eb394eaa8dcc9e37">每个子层都采用残差连接和层归一化：LayerNorm(x + Sublayer(x))，这有助于训练更深的网络和避免梯度消失问题。</p></section><section class="jsx-eb394eaa8dcc9e37 decoder-section"><h2 class="jsx-eb394eaa8dcc9e37">解码器详解</h2><p class="jsx-eb394eaa8dcc9e37">每个解码器层包含三个子层：</p><ol class="jsx-eb394eaa8dcc9e37"><li class="jsx-eb394eaa8dcc9e37"><strong class="jsx-eb394eaa8dcc9e37">掩蔽多头自注意力层</strong>：防止解码器关注后续位置的信息，保持自回归特性</li><li class="jsx-eb394eaa8dcc9e37"><strong class="jsx-eb394eaa8dcc9e37">编码器-解码器注意力层</strong>：让解码器关注输入序列的相关部分</li><li class="jsx-eb394eaa8dcc9e37"><strong class="jsx-eb394eaa8dcc9e37">前馈神经网络</strong>：与编码器中相同的前馈网络</li></ol><p class="jsx-eb394eaa8dcc9e37">解码器同样使用残差连接和层归一化，但在自注意力层中加入了掩码机制，确保位置i的预测只依赖于位置小于i的已知输出。</p></section><div class="jsx-eb394eaa8dcc9e37 navigation-links"><a class="nav-link" href="/transformer-paper-site/">返回首页</a></div></article></div></main></div><!--$--><!--/$--><!--$--><!--/$--><script src="/transformer-paper-site/_next/static/chunks/webpack-1c2f0ab7e589b28f.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[831,[],\"\"]\n3:I[8307,[],\"\"]\n4:I[9226,[],\"ClientPageRoot\"]\n5:I[9355,[\"91\",\"static/chunks/91-e2c009cf22ee4bab.js\",\"690\",\"static/chunks/690-1ea658c9bde215d9.js\",\"952\",\"static/chunks/app/architecture/page-c0294095c88c3613.js\"],\"default\"]\n8:I[9757,[],\"MetadataBoundary\"]\na:I[9757,[],\"OutletBoundary\"]\nd:I[8475,[],\"AsyncMetadataOutlet\"]\nf:I[9757,[],\"ViewportBoundary\"]\n11:I[8458,[],\"\"]\n:HL[\"/transformer-paper-site/_next/static/css/074657df094ca3ac.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"7PSQnb5Kq0UG_l8rY0OUI\",\"p\":\"/transformer-paper-site\",\"c\":[\"\",\"architecture\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"architecture\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/transformer-paper-site/_next/static/css/074657df094ca3ac.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"zh-CN\",\"children\":[\"$\",\"body\",null,{\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[\"architecture\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"$L4\",null,{\"Component\":\"$5\",\"searchParams\":{},\"params\":{},\"promises\":[\"$@6\",\"$@7\"]}],[\"$\",\"$L8\",null,{\"children\":\"$L9\"}],null,[\"$\",\"$La\",null,{\"children\":[\"$Lb\",\"$Lc\",[\"$\",\"$Ld\",null,{\"promise\":\"$@e\"}]]}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"sn1l44wnmpvDXHGHUEfvw\",{\"children\":[[\"$\",\"$Lf\",null,{\"children\":\"$L10\"}],null]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$11\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"12:\"$Sreact.suspense\"\n13:I[8475,[],\"AsyncMetadata\"]\n6:{}\n7:{}\n9:[\"$\",\"$12\",null,{\"fallback\":null,\"children\":[\"$\",\"$L13\",null,{\"promise\":\"$@14\"}]}]\n"])</script><script>self.__next_f.push([1,"c:null\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nb:null\n"])</script><script>self.__next_f.push([1,"14:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Transformer论文解析\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"《Attention Is All You Need》论文详细解析，了解Transformer架构的核心创新和工作原理\"}]],\"error\":null,\"digest\":\"$undefined\"}\ne:{\"metadata\":\"$14:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>